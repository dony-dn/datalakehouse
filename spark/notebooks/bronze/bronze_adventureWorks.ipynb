{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e47fa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 06:51:53,100 - INFO - Application started successfully.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info(\"Application started successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3788dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f304c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "\n",
    "NESSIE_URI = os.environ['NESSIE_URI']\n",
    "REF = \"etl\"\n",
    "FULL_PATH_TO_WAREHOUSE = os.environ['WAREHOUSE']\n",
    "AWS_S3_ENDPOINT = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "AWS_ACCESS_KEY = os.environ['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_KEY  = os.environ['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38aaef9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f4a02b690c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set pyspark configuration\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"process_dim_adventure_works\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .set(\"spark.executor.memory\", \"4g\") \\\n",
    "    .set(\"spark.driver.memory\", \"2g\") \\\n",
    "    .set(\"spark.sql.catalog.defaultCatalog\", \"nessie\") \\\n",
    "    .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\") \\\n",
    "    .set(\"spark.sql.catalog.nessie.uri\", NESSIE_URI) \\\n",
    "    .set(\"spark.sql.catalog.nessie.ref\", REF) \\\n",
    "    .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\") \\\n",
    "    .set(\"spark.sql.catalog.nessie.s3.endpoint\", AWS_S3_ENDPOINT) \\\n",
    "    .set(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\") \\\n",
    "    .set(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .set(\"spark.sql.catalog.nessie.warehouse\", FULL_PATH_TO_WAREHOUSE)\n",
    "\n",
    "# set MinIO config\n",
    "conf.set(\"fs.s3a.access.key\", AWS_ACCESS_KEY) \n",
    "conf.set(\"fs.s3a.secret.key\", AWS_SECRET_KEY) \n",
    "conf.set(\"fs.s3a.endpoint\", AWS_S3_ENDPOINT) \n",
    "conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\") \n",
    "conf.set(\"fs.s3a.path.style.access\", \"true\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5997e005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/30 06:51:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "2025-06-30 06:51:54,791 - INFO - Spark session started with Nessie and Iceberg configuration.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "logger.info(\"Spark session started with Nessie and Iceberg configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde826c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/30 06:51:57 WARN S3FileIO: Unclosed S3FileIO instance created by:\n",
      "\torg.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:444)\n",
      "\torg.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:402)\n",
      "\torg.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:349)\n",
      "\torg.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)\n",
      "\torg.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277)\n",
      "\torg.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331)\n",
      "\torg.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n",
      "\torg.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n",
      "\torg.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n",
      "\torg.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n",
      "\tscala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\torg.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n",
      "\torg.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:135)\n",
      "\torg.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:94)\n",
      "\torg.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:143)\n",
      "\torg.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:140)\n",
      "\torg.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:296)\n",
      "\tscala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tscala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tscala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\torg.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:296)\n",
      "\torg.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)\n",
      "\torg.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n",
      "\tscala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n",
      "\tscala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n",
      "\tscala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n",
      "\torg.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n",
      "\torg.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n",
      "\tscala.collection.immutable.List.foreach(List.scala:431)\n",
      "\torg.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n",
      "\torg.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n",
      "\torg.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\torg.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)\n",
      "\torg.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n",
      "\torg.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n",
      "\torg.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n",
      "\torg.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n",
      "\torg.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\torg.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\torg.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\torg.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\torg.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\torg.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\torg.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\torg.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\torg.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\torg.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\torg.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\torg.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\torg.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\torg.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\torg.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\torg.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
      "\torg.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "\torg.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\torg.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\torg.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "\torg.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\torg.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "\torg.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "\tjava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tjava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tjava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tjava.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tpy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tpy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tpy4j.Gateway.invoke(Gateway.java:282)\n",
      "\tpy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tpy4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tpy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tpy4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tjava.base/java.lang.Thread.run(Thread.java:840)\n",
      "2025-06-30 06:51:59,555 - INFO - namespace bronze exists: False                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   bronze|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create bronze in nessie catalog\n",
    "spark.sql(f\"USE REFERENCE main IN nessie\")\n",
    "namespace = \"bronze\"\n",
    "df = spark.sql(\"SHOW NAMESPACES in nessie\")\n",
    "namespace_exists = df.filter(df.namespace == namespace).count() > 0\n",
    "\n",
    "logger.info(f\"namespace {namespace} exists: {namespace_exists}\")\n",
    "if not namespace_exists:\n",
    "    spark.sql(f\"CREATE NAMESPACE nessie.{namespace}\")\n",
    "\n",
    "spark.sql(\"SHOW NAMESPACES in nessie\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cdda1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[refType: string, name: string, hash: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create etl branch\n",
    "spark.sql(f\"DROP BRANCH IF EXISTS {REF}\")\n",
    "spark.sql(f\"CREATE BRANCH {REF} IN nessie FROM main\")\n",
    "spark.sql(f\"LIST REFERENCES IN nessie\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fa73eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[refType: string, name: string, hash: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"USE nessie\")\n",
    "spark.sql(f\"USE REFERENCE {REF} IN nessie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "617a97a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/iceberg/notebooks/notebooks/lib/bronze_adventure_works.toml\n"
     ]
    }
   ],
   "source": [
    "import toml\n",
    "\n",
    "config_file = os.path.join(os.getcwd(), \"notebooks/lib/bronze_adventure_works.toml\")\n",
    "print(config_file)\n",
    "with open(config_file, \"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57a17b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 06:52:02,456 - INFO - Processing bronze_product...\n",
      "2025-06-30 06:52:02,458 - INFO - Processing Bronze | Source: s3a://warehouse/staging/adventure_works/product.csv, Target Table: bronze.PRODUCT\n",
      "25/06/30 06:52:02 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/06/30 06:52:03 WARN S3FileIO: Unclosed S3FileIO instance created by:\n",
      "\torg.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:444)\n",
      "\torg.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:402)\n",
      "\torg.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:349)\n",
      "\torg.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)\n",
      "\torg.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277)\n",
      "\torg.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331)\n",
      "\torg.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n",
      "\torg.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.NessieCatalogBridge.setCurrentRefForSpark(NessieCatalogBridge.java:105)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.UseReferenceExec.runInternal(UseReferenceExec.scala:44)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.NessieExec.run(NessieExec.scala:34)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\torg.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\torg.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\torg.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\torg.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\torg.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\torg.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\torg.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\torg.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\torg.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\torg.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\torg.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\torg.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\torg.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\torg.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\torg.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\torg.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\torg.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\torg.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\torg.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
      "\torg.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "\torg.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\torg.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\torg.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "\torg.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\torg.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "\torg.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "\tjava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tjava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tjava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tjava.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tpy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tpy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tpy4j.Gateway.invoke(Gateway.java:282)\n",
      "\tpy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tpy4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tpy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tpy4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tjava.base/java.lang.Thread.run(Thread.java:840)\n",
      "2025-06-30 06:52:05,077 - INFO - Data read from s3a://warehouse/staging/adventure_works/product.csv with schema: StructType([StructField('ProductId', StringType(), True), StructField('ProductName', StringType(), True), StructField('ProductSubcategory', StringType(), True), StructField('ProductCategory', StringType(), True), StructField('ProductDescription', StringType(), True), StructField('Color', StringType(), True), StructField('StandardCost', StringType(), True), StructField('rn', StringType(), True), StructField('file_path', StringType(), True), StructField('ingest_timestamp', StringType(), True), StructField('ingest_date', DateType(), True)])\n",
      "2025-06-30 06:52:05,298 - INFO - Table bronze.PRODUCT does not exist. Creating it.\n",
      "2025-06-30 06:52:09,540 - INFO - Table bronze.PRODUCT created successfully.     \n",
      "25/06/30 06:52:09 WARN NessieUtil: The Iceberg property 'gc.enabled' and/or 'write.metadata.delete-after-commit.enabled' is enabled on table 'bronze.PRODUCT' in NessieCatalog. This will likely make data in other Nessie branches and tags and in earlier, historical Nessie commits inaccessible. The recommended setting for those properties is 'false'. Use the 'nessie-gc' tool for Nessie reference-aware garbage collection.\n",
      "2025-06-30 06:52:09,816 - INFO - expiring old snapshots for bronze.PRODUCT\n",
      "2025-06-30 06:53:02,361 - INFO - removing orphan files for bronze.PRODUCT       \n",
      "25/06/30 06:53:02 WARN NessieUtil: The Iceberg property 'gc.enabled' and/or 'write.metadata.delete-after-commit.enabled' is enabled on table 'bronze.PRODUCT' in NessieCatalog. This will likely make data in other Nessie branches and tags and in earlier, historical Nessie commits inaccessible. The recommended setting for those properties is 'false'. Use the 'nessie-gc' tool for Nessie reference-aware garbage collection.\n",
      "2025-06-30 06:53:04,618 - INFO - Processing bronze_customer...                  \n",
      "2025-06-30 06:53:04,619 - INFO - Processing Bronze | Source: s3a://warehouse/staging/adventure_works/customer.csv, Target Table: bronze.CUSTOMER\n",
      "2025-06-30 06:53:04,908 - INFO - Data read from s3a://warehouse/staging/adventure_works/customer.csv with schema: StructType([StructField('UserId', StringType(), True), StructField('FirstName', StringType(), True), StructField('LastName', StringType(), True), StructField('BirthDate', StringType(), True), StructField('MaritalStatus', StringType(), True), StructField('Gender', StringType(), True), StructField('YearlyIncome', StringType(), True), StructField('Education', StringType(), True), StructField('Occupation', StringType(), True), StructField('HouseOwnerFlag', StringType(), True), StructField('TotalChildren', StringType(), True), StructField('NumberChildrenAtHome', StringType(), True), StructField('NumberCarsOwned', StringType(), True), StructField('DateFirstPurchase', StringType(), True), StructField('CommuteDistance', StringType(), True), StructField('file_path', StringType(), True), StructField('ingest_timestamp', StringType(), True), StructField('ingest_date', DateType(), True)])\n",
      "2025-06-30 06:53:05,071 - INFO - Table bronze.CUSTOMER does not exist. Creating it.\n",
      "2025-06-30 06:53:06,201 - INFO - Table bronze.CUSTOMER created successfully.\n",
      "25/06/30 06:53:06 WARN NessieUtil: The Iceberg property 'gc.enabled' and/or 'write.metadata.delete-after-commit.enabled' is enabled on table 'bronze.CUSTOMER' in NessieCatalog. This will likely make data in other Nessie branches and tags and in earlier, historical Nessie commits inaccessible. The recommended setting for those properties is 'false'. Use the 'nessie-gc' tool for Nessie reference-aware garbage collection.\n",
      "2025-06-30 06:53:06,354 - INFO - expiring old snapshots for bronze.CUSTOMER\n",
      "2025-06-30 06:53:07,515 - INFO - removing orphan files for bronze.CUSTOMER      \n",
      "2025-06-30 06:53:08,828 - INFO - Processing bronze_currency...\n",
      "2025-06-30 06:53:08,829 - INFO - Processing Bronze | Source: s3a://warehouse/staging/adventure_works/currency.csv, Target Table: bronze.CURRENCY\n",
      "2025-06-30 06:53:09,173 - INFO - Data read from s3a://warehouse/staging/adventure_works/currency.csv with schema: StructType([StructField('CurrencyAlternateKey', StringType(), True), StructField('CurrencyName', StringType(), True), StructField('file_path', StringType(), True), StructField('ingest_timestamp', StringType(), True), StructField('ingest_date', DateType(), True)])\n",
      "2025-06-30 06:53:09,224 - INFO - Table bronze.CURRENCY does not exist. Creating it.\n",
      "2025-06-30 06:53:09,616 - INFO - Table bronze.CURRENCY created successfully.\n",
      "25/06/30 06:53:09 WARN NessieUtil: The Iceberg property 'gc.enabled' and/or 'write.metadata.delete-after-commit.enabled' is enabled on table 'bronze.CURRENCY' in NessieCatalog. This will likely make data in other Nessie branches and tags and in earlier, historical Nessie commits inaccessible. The recommended setting for those properties is 'false'. Use the 'nessie-gc' tool for Nessie reference-aware garbage collection.\n",
      "2025-06-30 06:53:09,757 - INFO - expiring old snapshots for bronze.CURRENCY\n",
      "2025-06-30 06:53:10,702 - INFO - removing orphan files for bronze.CURRENCY\n",
      "2025-06-30 06:53:11,912 - INFO - Processing bronze_sales...\n",
      "2025-06-30 06:53:11,913 - INFO - Processing Bronze | Source: s3a://warehouse/staging/adventure_works/sales.csv, Target Table: bronze.SALES\n",
      "2025-06-30 06:53:12,167 - INFO - Data read from s3a://warehouse/staging/adventure_works/sales.csv with schema: StructType([StructField('SalesOrderNumber', StringType(), True), StructField('SalesOrderLineNumber', StringType(), True), StructField('ProductId', StringType(), True), StructField('CustomerUsername', StringType(), True), StructField('OrderDate', StringType(), True), StructField('DueDate', StringType(), True), StructField('ShipDate', StringType(), True), StructField('SalesTerritoryRegion', StringType(), True), StructField('SalesTerritoryCountry', StringType(), True), StructField('SalesTerritoryContinent', StringType(), True), StructField('OrderQuantity', StringType(), True), StructField('SalesAmount', StringType(), True), StructField('TaxAmt', StringType(), True), StructField('Freight', StringType(), True), StructField('Currency', StringType(), True), StructField('AverageRate', StringType(), True), StructField('file_path', StringType(), True), StructField('ingest_timestamp', StringType(), True), StructField('ingest_date', DateType(), True)])\n",
      "2025-06-30 06:53:12,276 - INFO - Table bronze.SALES does not exist. Creating it.\n",
      "2025-06-30 06:53:13,429 - INFO - Table bronze.SALES created successfully.\n",
      "25/06/30 06:53:13 WARN NessieUtil: The Iceberg property 'gc.enabled' and/or 'write.metadata.delete-after-commit.enabled' is enabled on table 'bronze.SALES' in NessieCatalog. This will likely make data in other Nessie branches and tags and in earlier, historical Nessie commits inaccessible. The recommended setting for those properties is 'false'. Use the 'nessie-gc' tool for Nessie reference-aware garbage collection.\n",
      "2025-06-30 06:53:13,573 - INFO - expiring old snapshots for bronze.SALES\n",
      "2025-06-30 06:53:14,457 - INFO - removing orphan files for bronze.SALES\n",
      "2025-06-30 06:53:15,551 - INFO - Processing bronze_currency_rate...\n",
      "2025-06-30 06:53:15,552 - INFO - Processing Bronze | Source: s3a://warehouse/staging/adventure_works/currency_rate.csv, Target Table: bronze.CURRENCY_RATE\n",
      "2025-06-30 06:53:15,744 - INFO - Data read from s3a://warehouse/staging/adventure_works/currency_rate.csv with schema: StructType([StructField('currency', StringType(), True), StructField('date', StringType(), True), StructField('rate', StringType(), True), StructField('file_path', StringType(), True), StructField('ingest_timestamp', StringType(), True), StructField('ingest_date', DateType(), True)])\n",
      "2025-06-30 06:53:15,800 - INFO - Table bronze.CURRENCY_RATE does not exist. Creating it.\n",
      "2025-06-30 06:53:16,241 - INFO - Table bronze.CURRENCY_RATE created successfully.\n",
      "25/06/30 06:53:16 WARN NessieUtil: The Iceberg property 'gc.enabled' and/or 'write.metadata.delete-after-commit.enabled' is enabled on table 'bronze.CURRENCY_RATE' in NessieCatalog. This will likely make data in other Nessie branches and tags and in earlier, historical Nessie commits inaccessible. The recommended setting for those properties is 'false'. Use the 'nessie-gc' tool for Nessie reference-aware garbage collection.\n",
      "2025-06-30 06:53:16,372 - INFO - expiring old snapshots for bronze.CURRENCY_RATE\n",
      "2025-06-30 06:53:17,190 - INFO - removing orphan files for bronze.CURRENCY_RATE\n"
     ]
    }
   ],
   "source": [
    "for item in [item for item in config if config[item][\"ingestion\"][\"enabled\"]]:\n",
    "    logger.info(f\"Processing {item}...\")\n",
    "    source = os.path.join(FULL_PATH_TO_WAREHOUSE, config[item][\"ingestion\"][\"source\"])\n",
    "    target_table = namespace + \".\" + config[item][\"ingestion\"][\"target_table\"].upper()\n",
    "    column_mapping = config[item][\"column_mapping\"]\n",
    "    logger.info(f\"Processing Bronze | Source: {source}, Target Table: {target_table}\")\n",
    "\n",
    "    # Read the data from the source\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", True) \\\n",
    "        .csv(source)\n",
    "    logger.info(f\"Data read from {source} with schema: {df.schema}\")\n",
    "    \n",
    "    # Rename columns based on the mapping, handling comma-separated source columns\n",
    "    for target_col, source_cols in column_mapping.items():\n",
    "        source_col_list = [col.strip() for col in source_cols.split(\",\")]\n",
    "        for src in source_col_list:\n",
    "            if src in df.columns:\n",
    "                df = df.withColumnRenamed(src, target_col)\n",
    "                break  # Stop after the first match\n",
    "\n",
    "    # Check if the target table exists\n",
    "    if not spark.catalog.tableExists(f\"nessie.{target_table}\"):\n",
    "        logger.info(f\"Table {target_table} does not exist. Creating it.\")\n",
    "        # Create the table with the specified schema and partitioning\n",
    "        df.writeTo(f\"nessie.{target_table}\") \\\n",
    "            .using(\"iceberg\") \\\n",
    "            .partitionedBy(\"ingest_date\") \\\n",
    "            .createOrReplace()\n",
    "        logger.info(f\"Table {target_table} created successfully.\")\n",
    "    else:\n",
    "        logger.info(f\"Table {target_table} already exists.\")\n",
    "        # If the table exists, check for new columns and add them if necessary\n",
    "        existing_cols = set(spark.table(f\"nessie.{target_table}\").columns)\n",
    "        df_cols = set(df.columns)\n",
    "        \n",
    "        # reorder df columns to match the target table schema\n",
    "        missing_cols = existing_cols - df_cols\n",
    "        for col in missing_cols:\n",
    "            df = df.withColumn(col, lit(None).cast(dict(spark.table(f\"nessie.{target_table}\").dtypes)[col]))\n",
    "            logger.info(f\"Added missing column {col} with null values to DataFrame\")\n",
    "        \n",
    "        # Overwrite only the partitions present in the DataFrame (by ingest_date)\n",
    "        df.writeTo(f\"nessie.{target_table}\") \\\n",
    "            .using(\"iceberg\") \\\n",
    "            .overwritePartitions()\n",
    "        \n",
    "    # enable iceberg garbage collection\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE nessie.{target_table} SET TBLPROPERTIES ('gc.enabled'='true')\n",
    "    \"\"\")\n",
    "    \n",
    "    # Data retention: expire old snapshots and remove orphan files\n",
    "    expire_ts = (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    orphan_ts = (datetime.now() - timedelta(days=7)).strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    \n",
    "    logger.info(f'expiring old snapshots for {target_table}')\n",
    "    spark.sql(f\"\"\"\n",
    "        CALL nessie.system.expire_snapshots(\n",
    "            table => '{target_table}',\n",
    "            older_than => TIMESTAMP '{expire_ts}',\n",
    "            retain_last => 1\n",
    "        )\n",
    "    \"\"\")\n",
    "    logger.info(f'removing orphan files for {target_table}')\n",
    "    spark.sql(f\"\"\"\n",
    "        CALL nessie.system.remove_orphan_files(\n",
    "            table => '{target_table}',\n",
    "            older_than => TIMESTAMP '{orphan_ts}'\n",
    "        )\n",
    "    \"\"\")\n",
    "        \n",
    "    # disable iceberg garbage collection\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE nessie.{target_table} SET TBLPROPERTIES ('gc.enabled'='false')\n",
    "    \"\"\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d36ec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+\n",
      "|refType|name|                hash|\n",
      "+-------+----+--------------------+\n",
      "| Branch| etl|bf7e24b9020fb1c5f...|\n",
      "| Branch|main|0b2b7d5a8140efd80...|\n",
      "+-------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"LIST REFERENCES IN nessie\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b5caaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|name|                hash|\n",
      "+----+--------------------+\n",
      "|main|8c17b6e462babdc50...|\n",
      "+----+--------------------+\n",
      "\n",
      "+------+\n",
      "|status|\n",
      "+------+\n",
      "|    OK|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"MERGE BRANCH {REF} INTO main IN nessie\").show()\n",
    "spark.sql(f\"DROP BRANCH {REF} IN nessie\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff2c6e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+\n",
      "|refType|name|                hash|\n",
      "+-------+----+--------------------+\n",
      "| Branch|main|8c17b6e462babdc50...|\n",
      "+-------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE REFERENCE main IN nessie\")\n",
    "spark.sql(\"LIST REFERENCES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b12870a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 06:53:19,398 - INFO - Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "logger.info(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
